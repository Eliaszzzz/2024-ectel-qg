{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nLlama 2 13B-Chat Generation.\nThis section defines a function to save a list of Q&A dictionaries to a JSON file.\nThe list is sorted by file number and slide number before being saved.\nThis function is used in other sections of the code.\n\"\"\"\n\nimport os\nimport json\n\ndef save_json(filename, qa_list):\n    \"\"\"\n    Save the QA list of dictionaries to a JSON file.\n    \"\"\"\n    # Order qa_list by slide_num and file_num\n    qa_list.sort(key=lambda x: (x['file_num'], x['slide_num']))\n\n    # Specify the directory where you want to save the file\n    output_dir = '/kaggle/working/'  # You can adjust this path as needed\n    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n    file_path = os.path.join(output_dir, filename)  # Full file path\n\n    # Open the file in write mode\n    with open(filename, \"w\") as file:\n        # Write the data to the file\n        json.dump(qa_list, file, ensure_ascii=False, indent=4)\n    print(f\"Saved {len(qa_list)} elements to {filename}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:25:07.750496Z","iopub.execute_input":"2024-11-17T12:25:07.750903Z","iopub.status.idle":"2024-11-17T12:25:07.758590Z","shell.execute_reply.started":"2024-11-17T12:25:07.750868Z","shell.execute_reply":"2024-11-17T12:25:07.757579Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"'''\nThis section sets up the Llama 2 model for text generation.\nIt loads the model and tokenizer from the Hugging Face Model Hub,\nsets the model to evaluation mode, and initializes the text generation pipeline.\n'''\n\nimport bitsandbytes\nimport torch\nimport transformers\nfrom torch import cuda, bfloat16\nfrom huggingface_hub import HfApi\nfrom transformers import AutoModelForCausalLM\n\n# Specify the model ID\nmodel_id = 'meta-llama/Llama-2-13b-chat-hf'\n\n# Set the device to CUDA if available, otherwise use CPU\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\ndevice = torch.device(\"cuda:0\")\n\n# Configure the BitsAndBytes quantization settings for the model\nbnb_config = transformers.BitsAndBytesConfig(\n    bnb_4bit_quant_type='nf4',  # Use 4-bit quantization\n    bnb_4bit_use_double_quant=True,  # Use double quantization\n    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 data type for computations\n)\n\n# Get the Hugging Face authentication token from environment variables\nhf_auth = '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'\n\n# Load the model configuration from Hugging Face Model Hub\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,  # The model ID\n    use_auth_token=hf_auth  # The authentication token\n)\n\n# Load the model from Hugging Face Model Hub\nllama2_model = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,  # The model ID\n    trust_remote_code=True,  # Trust the remote code (be careful with this setting)\n    config=model_config,  # The model configuration\n    quantization_config=bnb_config,  # The quantization configuration\n    device_map='auto',  # Automatically map the model to the device\n    torch_dtype=torch.float16,  # Use float16 data type for the model\n    use_auth_token=hf_auth,  # The authentication token\n)\n\n# Set the model to evaluation mode\nllama2_model.eval()\n\n# Print the device the model is loaded on\nprint(f\"Model loaded on {device}\")\n\n# Load the tokenizer from Hugging Face Model Hub\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_id,  # The model ID\n    use_auth_token=hf_auth  # The authentication token\n)\n\n# Initialize the text generation pipeline\ngenerate_text = transformers.pipeline(\n    model=llama2_model,  # The model\n    tokenizer=tokenizer,  # The tokenizer\n    return_full_text=False,  # Return the full text or just the generated part\n    task='text-generation',  # The task is text generation\n    temperature=0.05,  # The 'randomness' of the outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=256,  # The maximum number of tokens to generate in the output\n    repetition_penalty=1.1  # The penalty for repetition in the output\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:39:04.396285Z","iopub.execute_input":"2024-11-17T11:39:04.396645Z","iopub.status.idle":"2024-11-17T11:41:11.260860Z","shell.execute_reply.started":"2024-11-17T11:39:04.396613Z","shell.execute_reply":"2024-11-17T11:41:11.259841Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:991: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74369a6899f425b9e21ef9cbe1ac028"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\"\"\"\nThis section generates questions and answers based on the content of PDF files. \nIt reads the PDF files, extracts the text, generates answers and questions \nusing the Llama 2 model, and saves the results to a list.\n\"\"\"\n\n# Import necessary libraries\nimport PyPDF2\nfrom PyPDF2 import PdfReader\nimport re\nfrom tqdm import tqdm\n\n\ndef llama2_prompt(sys_prompt: str = \"\", instruction_list: str = [\"\"], prev_answer_list: list = []):\n    \"\"\"\n    Returns a prompt in the format required by the Llama 2 model\n    \"\"\"\n    prompt = f\"<s>[INST] <<SYS>>\\n{sys_prompt}\\n<</SYS>>\\n\\n\"\n    for i, instruction in enumerate(instruction_list):\n        prompt += f\"{instruction} [/INST] \"\n        if i < len(prev_answer_list):\n            prompt += f\"{prev_answer_list[i]} </s><s>[INST] \"\n    return prompt\n\n\n# Define system prompt list\nsys_prompt_list = [\n    f\"\"\"You are an expert of \\\"Deep Learning\\\". You have to generate questions in order to challenge students about the subject material. Your answers should be short, concise, fundamental, clear and relevant for the subject.\\n\"\"\"]\n\n# Define history list and QA list\nhistory_list = []\nqa_list = []\n\n# Define slide list\nslide_list = [(1, 30), (2, 8), (3, 33), (4, 8), (5, 41), (6, 30), (7, 60), (8, 20), (9, 22), (10, 42), (11, 11)]\n\n# Loop through each slide\nfor (file_num, slide_num) in tqdm(slide_list):\n    # Read the PDF file\n    filename = f'/kaggle/input/material/dl_mt/{file_num}.pdf'\n    reader = PdfReader(filename)\n    file_text = [page.extract_text() for page in reader.pages]\n\n    # Define context\n    context = ''\n    context_range = 2\n    for i in range(-context_range, context_range + 1):\n        try:\n            context += \"\\n\" + file_text[slide_num + i] + \"\\n\"\n        except:\n            print(\n                f\"Warning (index out of bounds, slide doesn't exist). Omitting: file={filename}, slide_num={slide_num + i + 1}\")\n\n    # Define text\n    text = file_text[slide_num]\n\n    # Define answers\n    answers = []\n    ans_prompt_list = [\n        f\"Considering the following text: \\'\\'\\'{context}\\'\\'\\'\\n\\nSeparate the ideas of the following extract and format them as an explanation sentences using the exact expressions in the text, avoiding questions:\\n\\nExtract: \\'\\'\\'{text}\\'\\'\\'\\n\\n1)\"]\n\n    # Generate text for each prompt in ans_prompt_list\n    for ans_prompt in ans_prompt_list:\n        res = generate_text(llama2_prompt(sys_prompt=sys_prompt_list[0], instruction_list=[ans_prompt]))\n        output_text = res[0][\"generated_text\"]\n        # print(re.sub('\\n+', '\\n', output_text))\n\n        # Divide the string into substrings using the Llama 2 pattern \"1) \", \"2) \", etc.\n        subchains = re.split(r\"\\n\\d+\\. \", output_text)[1:]\n\n        # Erase the number at the end\n        subchains = [re.sub(r\"\\n\\d+$\", '', subchain) for subchain in subchains]\n        answers += subchains\n\n    # Generate questions for each answer\n    for sys_prompt in sys_prompt_list:\n        for answer in answers:\n            prompt_list = [\n                f\"\"\"The topic is \\\"{file_text[1]} - {file_text[2]}\\\". Considering the following text:\\'\\'\\'{context}\\'\\'\\'. Extract: \\'\\'\\'{answer}\\'\\'\\'. Please generate a very short question which answer is contained in the extract.\"\"\"]\n\n            # Generate text for each prompt in prompt_list\n            for prompt in prompt_list:\n                res = generate_text(llama2_prompt(sys_prompt=sys_prompt, instruction_list=[prompt]))\n                match = re.search(r\"\\b.*?\\?\", res[0][\"generated_text\"])\n                match_text = match.group() if match else \"\"\n                match_text = match_text.replace(\"Question: \", \"\")\n\n                # Append to qa_list\n                qa_list.append({'file_num': file_num,\n                                'slide_num': slide_num,\n                                'slide_text': text,\n                                'extended_context': context,\n                                'gen_answer': answer,\n                                'gen_question_llama_2': match_text})\n\n# Save the QA list to a JSON file\nsave_json(\"qa_list.json\", qa_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T11:41:11.262568Z","iopub.execute_input":"2024-11-17T11:41:11.262917Z","iopub.status.idle":"2024-11-17T12:07:46.758994Z","shell.execute_reply.started":"2024-11-17T11:41:11.262884Z","shell.execute_reply":"2024-11-17T12:07:46.758044Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/11 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n  9%|▉         | 1/11 [03:36<36:01, 216.12s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n 36%|███▋      | 4/11 [07:20<11:05, 95.07s/it] This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n100%|██████████| 11/11 [26:35<00:00, 145.04s/it]","output_type":"stream"},{"name":"stdout","text":"Saved 37 elements to qa_list.json.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"save_json(\"qa_list.json\", qa_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:26:07.010782Z","iopub.execute_input":"2024-11-17T12:26:07.011184Z","iopub.status.idle":"2024-11-17T12:26:07.020987Z","shell.execute_reply.started":"2024-11-17T12:26:07.011149Z","shell.execute_reply":"2024-11-17T12:26:07.020076Z"}},"outputs":[{"name":"stdout","text":"Saved 37 elements to qa_list.json.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\"\"\"\nThis section cleans up the memory by deleting the model and text generation pipeline, \ntriggering the garbage collector, and clearing the GPU memory cache.\n\"\"\"\n\nimport gc\n\n# Delete the 'generate_text' and the 'llama2_model' objects from memory\ndel generate_text\ndel llama2_model\n\n# Manually trigger Python's garbage collector\ngc.collect()\n\n# Clear the memory cache in PyTorch for the GPU\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:07:46.760494Z","iopub.execute_input":"2024-11-17T12:07:46.760825Z","iopub.status.idle":"2024-11-17T12:07:47.433743Z","shell.execute_reply.started":"2024-11-17T12:07:46.760791Z","shell.execute_reply":"2024-11-17T12:07:47.432703Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\"\"\"\nGPT 3.5 Turbo Generation\nThis section loops through each Q&A pair in the list, \nextracts the necessary information, defines the prompt for the OpenAI API, \nmakes a request to the API to generate a question, \nextracts the generated question from the response, \nadds the question to the Q&A pair, \nand appends the updated pair to the list.\n\"\"\"\n\nimport os\nimport json\nimport openai\nfrom tqdm import tqdm\n\n# Set API\nopenai.api_key = \"sk-0FjRts5Wg2t8iOVAA11c2899A9Fc45C1Af7aB7C4B5C6D4E2\"\nopenai.base_url = \"https://free.v36.cm\"\nopenai.default_headers = {\"x-foo\": \"true\"}\n\n# Define system prompt (role)\nsys_prompt = \"\"\"You are an expert of \"Deep Learning\". You have to generate questions in order to challenge students about the subject material. Your answers should be short, concise, fundamental, clear and relevant for the subject.\\n\\n\"\"\"\n\n# Open the file in read mode and load the JSON data\nwith open(\"qa_list.json\", \"r\") as file:\n    qa_list = json.load(file)\n\n# Initialize an empty list to store the updated QA pairs\nqa_list_updated = []\n\n# Loop through each QA pair in the list\nfor qa in tqdm(qa_list):\n    # Extract the necessary information from the QA pair\n    file_num = qa['file_num']\n    slide_num = qa['slide_num']\n    text = qa['slide_text']\n    context = qa['extended_context']\n    answer = qa['gen_answer']\n\n    # Define the prompt for the OpenAI API\n    prompt = f\"\"\"Considering the following text:\\'\\'\\'{context}\\'\\'\\'. Extract: \\'\\'\\'{answer}\\'\\'\\'. Please generate a short question which answer is contained in the extract.\\n\\nQ:\"\"\"\n\n    # Make a request to the OpenAI API\n    response = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": sys_prompt},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n\n    # Extract the generated question from the response\n    output_text = response.choices[0].message.content\n\n    # Add the generated question to the QA pair\n    qa['gen_question_gpt-3.5-turbo'] = output_text\n\n    # Append the updated QA pair to the list\n    qa_list_updated.append(qa)\n\n# Save the QA updated list to a JSON file\nsave_json(\"qa_list.json\", qa_list_updated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:07:47.435890Z","iopub.execute_input":"2024-11-17T12:07:47.436237Z","iopub.status.idle":"2024-11-17T12:07:47.493564Z","shell.execute_reply.started":"2024-11-17T12:07:47.436204Z","shell.execute_reply":"2024-11-17T12:07:47.492393Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Set API\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"],"ename":"ModuleNotFoundError","evalue":"No module named 'openai'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"\"\"\"\nFlan T5 XXL Generation\nThis section sets up the T5 model and the text generation pipeline.\n\"\"\"\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n\n# Specify the name of the model to be used\nmodel_name = 'google/flan-t5-xxl'\n\n# Initialize the tokenizer for the specified model\n# The tokenizer will return tensors in PyTorch format\nt5_tokenizer = T5Tokenizer.from_pretrained(\n    model_name,\n    return_tensors=\"pt\")\n\n# Initialize the model for the specified model\n# The model will be loaded to the device specified by 'device_map'\n# The model will use the 'bfloat16' data type for its tensors\nt5_model = T5ForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\n# Initialize the text generation pipeline\n# The pipeline uses the specified model and tokenizer\n# The task is set to 'text2text-generation'\n# The repetition penalty is set to 1.1 to prevent the output from repeating\ngenerate_text = pipeline(\n    model=t5_model, tokenizer=t5_tokenizer,\n    task='text2text-generation',\n    repetition_penalty=1.1  # without this output begins repeating\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:07:47.494455Z","iopub.status.idle":"2024-11-17T12:07:47.494809Z","shell.execute_reply.started":"2024-11-17T12:07:47.494636Z","shell.execute_reply":"2024-11-17T12:07:47.494653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nThis section uses the T5 model to generate questions \nbased on the context and answer of each Q&A pair. \nThe generated questions are added to the Q&A pairs \nand the updated pairs are stored in a new list.\n\"\"\"\n\nimport json\nfrom tqdm import tqdm\n\n# Open the JSON file and load the data\nwith open(\"qa_list.json\", \"r\") as file:\n    qa_list = json.load(file)\n\n# Initialize an empty list to store the updated QA pairs\nqa_list_updated = []\n\n# Iterate over each QA pair in the list\nfor qa in tqdm(qa_list):\n    # Extract the necessary information from the QA pair\n    file_num = qa['file_num']\n    slide_num = qa['slide_num']\n    text = qa['slide_text']\n    context = qa['extended_context']\n    answer = qa['gen_answer']\n\n    # Generate a prompt for the text generation model\n    prompt = f\"Write a question about the context:{context}. For this answer: {answer}. Question:\"\n\n    # Use the text generation model to generate a question\n    res = generate_text([prompt])\n\n    # Extract the generated question from the model's response\n    generated_question = res[0][\"generated_text\"]\n\n    # Add the generated question to the QA pair\n    qa['gen_question_flan_t5_xxl'] = generated_question\n\n    # Append the updated QA pair to the list\n    qa_list_updated.append(qa)\n\n# Save the QA updated list to a JSON file\nsave_json(\"qa_list.json\", qa_list_updated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:07:47.496206Z","iopub.status.idle":"2024-11-17T12:07:47.496605Z","shell.execute_reply.started":"2024-11-17T12:07:47.496419Z","shell.execute_reply":"2024-11-17T12:07:47.496438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nThis section deletes the text generation pipeline and the T5 model from memory, \ntriggers Python's garbage collector, and clears the memory cache in PyTorch for the GPU.\n\"\"\"\n\nimport gc\n\n# Delete the 'generate_text' and the 't5_model' objects from memory\ndel generate_text\ndel t5_model\n\n# Manually trigger Python's garbage collector\ngc.collect()\n\n# Clear the memory cache in PyTorch for the GPU\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T12:07:47.497615Z","iopub.status.idle":"2024-11-17T12:07:47.497969Z","shell.execute_reply.started":"2024-11-17T12:07:47.497789Z","shell.execute_reply":"2024-11-17T12:07:47.497807Z"}},"outputs":[],"execution_count":null}]}