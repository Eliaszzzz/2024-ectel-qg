{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 13B-Chat Generation\n",
    "This section defines a function to save a list of Q&A dictionaries to a JSON file. The list is sorted by file number and slide number before being saved. This function is used in other sections of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_json(filename, qa_list):\n",
    "    \"\"\"\n",
    "    Save the QA list of dictionaries to a JSON file.\n",
    "    \"\"\"\n",
    "    # Order qa_list by slide_num and file_num\n",
    "    qa_list.sort(key=lambda x: (x['file_num'], x['slide_num']))\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(filename, \"w\") as file:\n",
    "        # Write the data to the file\n",
    "        json.dump(qa_list, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(qa_list)} elements to {filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section sets up the Llama 2 model for text generation. It loads the model and tokenizer from the Hugging Face Model Hub, sets the model to evaluation mode, and initializes the text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "# Specify the model ID\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "# Set the device to CUDA if available, otherwise use CPU\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Configure the BitsAndBytes quantization settings for the model\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    bnb_4bit_quant_type='nf4',  # Use 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 data type for computations\n",
    ")\n",
    "\n",
    "# Get the Hugging Face authentication token from environment variables\n",
    "hf_auth = os.getenv('HF_AUTH_TOKEN')\n",
    "\n",
    "# Load the model configuration from Hugging Face Model Hub\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,  # The model ID\n",
    "    use_auth_token=hf_auth  # The authentication token\n",
    ")\n",
    "\n",
    "# Load the model from Hugging Face Model Hub\n",
    "llama2_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,  # The model ID\n",
    "    trust_remote_code=True,  # Trust the remote code (be careful with this setting)\n",
    "    config=model_config,  # The model configuration\n",
    "    quantization_config=bnb_config,  # The quantization configuration\n",
    "    device_map='auto',  # Automatically map the model to the device\n",
    "    torch_dtype=torch.float16,  # Use float16 data type for the model\n",
    "    use_auth_token=hf_auth  # The authentication token\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "llama2_model.eval()\n",
    "\n",
    "# Print the device the model is loaded on\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Load the tokenizer from Hugging Face Model Hub\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,  # The model ID\n",
    "    use_auth_token=hf_auth  # The authentication token\n",
    ")\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    model=llama2_model,  # The model\n",
    "    tokenizer=tokenizer,  # The tokenizer\n",
    "    return_full_text=False,  # Return the full text or just the generated part\n",
    "    task='text-generation',  # The task is text generation\n",
    "    temperature=0.05,  # The 'randomness' of the outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=256,  # The maximum number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # The penalty for repetition in the output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section generates questions and answers based on the content of PDF files. It reads the PDF files, extracts the text, generates answers and questions using the Llama 2 model, and saves the results to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def llama2_prompt(sys_prompt: str = \"\", instruction_list: str = [\"\"], prev_answer_list: list = []):\n",
    "    \"\"\"\n",
    "    Returns a prompt in the format required by the Llama 2 model\n",
    "    \"\"\"\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\n{sys_prompt}\\n<</SYS>>\\n\\n\"\n",
    "    for i, instruction in enumerate(instruction_list):\n",
    "        prompt += f\"{instruction} [/INST] \"\n",
    "        if i < len(prev_answer_list):\n",
    "            prompt += f\"{prev_answer_list[i]} </s><s>[INST] \"\n",
    "    return prompt\n",
    "\n",
    "# Define system prompt list\n",
    "sys_prompt_list = [f\"\"\"You are an expert of \\\"Pattern Recognition\\\". You have to generate questions in order to challenge students about the subject material. Your answers should be short, concise, fundamental, clear and relevant for the subject.\\n\"\"\"]\n",
    "\n",
    "# Define history list and QA list\n",
    "history_list = []\n",
    "qa_list = []\n",
    "\n",
    "# Define slide list\n",
    "slide_list = [(6, 6), (8, 20), (7, 5), (9, 5), (6, 45), (3, 21), (2, 11), (2, 20), (6, 17), (5, 6), (7, 16), (5, 3), (7, 13), (2, 35), (8, 3), (3, 23), (7, 3), (9, 3), (9, 9), (6, 22), (7, 21)]\n",
    "\n",
    "# Loop through each slide\n",
    "for (file_num, slide_num) in tqdm(slide_list):\n",
    "    # Read the PDF file\n",
    "    filename = f'material/PR_0{file_num}.pdf'\n",
    "    reader = PdfReader(filename)\n",
    "    file_text = [page.extract_text() for page in reader.pages]\n",
    "\n",
    "    # Define context\n",
    "    context = ''\n",
    "    context_range = 2\n",
    "    for i in range(-context_range, context_range+1):\n",
    "        try:\n",
    "            context += \"\\n\" + file_text[slide_num+i] + \"\\n\"\n",
    "        except:\n",
    "            print(f\"Warning (index out of bounds, slide doesn't exist). Omitting: file={filename}, slide_num={slide_num+i+1}\")\n",
    "\n",
    "    # Define text\n",
    "    text = file_text[slide_num]\n",
    "\n",
    "    # Define answers\n",
    "    answers = []\n",
    "    ans_prompt_list = [f\"Considering the following text: \\'\\'\\'{context}\\'\\'\\'\\n\\nSeparate the ideas of the following extract and format them as an explanation sentences using the exact expressions in the text, avoiding questions:\\n\\nExtract: \\'\\'\\'{text}\\'\\'\\'\\n\\n1)\"]\n",
    "    \n",
    "    # Generate text for each prompt in ans_prompt_list\n",
    "    for ans_prompt in ans_prompt_list:\n",
    "        res = generate_text(llama2_prompt(sys_prompt=sys_prompt_list[0],instruction_list=[ans_prompt]))\n",
    "        output_text = res[0][\"generated_text\"]\n",
    "        # print(re.sub('\\n+', '\\n', output_text))\n",
    "        \n",
    "        # Divide the string into substrings using the Llama 2 pattern \"1) \", \"2) \", etc.\n",
    "        subchains = re.split(r\"\\n\\d+\\. \", output_text)[1:]\n",
    "        \n",
    "        # Erase the number at the end\n",
    "        subchains = [re.sub(r\"\\n\\d+$\", '', subchain) for subchain in subchains]\n",
    "        answers += subchains\n",
    "\n",
    "    # Generate questions for each answer\n",
    "    for sys_prompt in sys_prompt_list:\n",
    "        for answer in answers:\n",
    "            prompt_list = [f\"\"\"The topic is \\\"{file_text[1]} - {file_text[2]}\\\". Considering the following text:\\'\\'\\'{context}\\'\\'\\'. Extract: \\'\\'\\'{answer}\\'\\'\\'. Please generate a very short question which answer is contained in the extract.\"\"\"]\n",
    "\n",
    "            # Generate text for each prompt in prompt_list\n",
    "            for prompt in prompt_list:\n",
    "                res = generate_text(llama2_prompt(sys_prompt=sys_prompt,instruction_list=[prompt]))\n",
    "                match = re.search(r\"\\b.*?\\?\", res[0][\"generated_text\"])\n",
    "                match_text = match.group() if match else \"\"\n",
    "                match_text = match_text.replace(\"Question: \", \"\")\n",
    "                \n",
    "                # Append to qa_list\n",
    "                qa_list.append({'file_num': file_num, \n",
    "                                'slide_num': slide_num,\n",
    "                                'slide_text': text,\n",
    "                                'extended_context':  context,\n",
    "                                'gen_answer': answer,\n",
    "                                'gen_question_llama_2': match_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the QA list to a JSON file\n",
    "save_json(\"qa_list.json\", qa_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans up the memory by deleting the model and text generation pipeline, triggering the garbage collector, and clearing the GPU memory cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the 'generate_text' and the 'llama2_model' objects from memory\n",
    "del generate_text\n",
    "del llama2_model\n",
    "\n",
    "# Manually trigger Python's garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear the memory cache in PyTorch for the GPU\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 3.5 Turbo Generation\n",
    "This section loops through each Q&A pair in the list, extracts the necessary information, defines the prompt for the OpenAI API, makes a request to the API to generate a question, extracts the generated question from the response, adds the question to the Q&A pair, and appends the updated pair to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the OpenAI API key from environment variable\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Set the OpenAI API key\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Define system prompt (role)\n",
    "sys_prompt = \"\"\"You are an expert of \"Pattern Recognition\". You have to generate questions in order to challenge students about the subject material. Your answers should be short, concise, fundamental, clear and relevant for the subject.\\n\\n\"\"\"\n",
    "\n",
    "# Open the file in read mode and load the JSON data\n",
    "with open(\"qa_list.json\", \"r\") as file:\n",
    "    qa_list = json.load(file)\n",
    "\n",
    "# Initialize an empty list to store the updated QA pairs\n",
    "qa_list_updated = []\n",
    "\n",
    "# Loop through each QA pair in the list\n",
    "for qa in tqdm(qa_list):\n",
    "    # Extract the necessary information from the QA pair\n",
    "    file_num = qa['file_num']\n",
    "    slide_num = qa['slide_num']\n",
    "    text = qa['slide_text']\n",
    "    context = qa['extended_context']\n",
    "    answer = qa['gen_answer']\n",
    "\n",
    "    # Define the prompt for the OpenAI API\n",
    "    prompt = f\"\"\"Considering the following text:\\'\\'\\'{context}\\'\\'\\'. Extract: \\'\\'\\'{answer}\\'\\'\\'. Please generate a short question which answer is contained in the extract.\\n\\nQ:\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the generated question from the response\n",
    "    output_text = response.choices[0].message.content\n",
    "\n",
    "    # Add the generated question to the QA pair\n",
    "    qa['gen_question_gpt-3.5-turbo'] = output_text\n",
    "\n",
    "    # Append the updated QA pair to the list\n",
    "    qa_list_updated.append(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the QA updated list to a JSON file\n",
    "save_json(\"qa_list.json\", qa_list_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan T5 XXL Generation\n",
    "This section sets up the T5 model and the text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "\n",
    "# Specify the name of the model to be used\n",
    "model_name = 'google/flan-t5-xxl'\n",
    "\n",
    "# Initialize the tokenizer for the specified model\n",
    "# The tokenizer will return tensors in PyTorch format\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "# Initialize the model for the specified model\n",
    "# The model will be loaded to the device specified by 'device_map'\n",
    "# The model will use the 'bfloat16' data type for its tensors\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "# The pipeline uses the specified model and tokenizer\n",
    "# The task is set to 'text2text-generation'\n",
    "# The repetition penalty is set to 1.1 to prevent the output from repeating\n",
    "generate_text = pipeline(\n",
    "    model=t5_model, tokenizer=t5_tokenizer,\n",
    "    task='text2text-generation',\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses the T5 model to generate questions based on the context and answer of each Q&A pair. The generated questions are added to the Q&A pairs and the updated pairs are stored in a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"qa_list.json\", \"r\") as file:\n",
    "    qa_list = json.load(file)\n",
    "\n",
    "# Initialize an empty list to store the updated QA pairs\n",
    "qa_list_updated = []\n",
    "\n",
    "# Iterate over each QA pair in the list\n",
    "for qa in tqdm(qa_list):\n",
    "    # Extract the necessary information from the QA pair\n",
    "    file_num = qa['file_num']\n",
    "    slide_num = qa['slide_num']\n",
    "    text = qa['slide_text']\n",
    "    context = qa['extended_context']\n",
    "    answer = qa['gen_answer']\n",
    "\n",
    "    # Generate a prompt for the text generation model\n",
    "    prompt = f\"Write a question about the context:{context}. For this answer: {answer}. Question:\"\n",
    "\n",
    "    # Use the text generation model to generate a question\n",
    "    res = generate_text([prompt])\n",
    "\n",
    "    # Extract the generated question from the model's response\n",
    "    generated_question = res[0][\"generated_text\"]\n",
    "\n",
    "    # Add the generated question to the QA pair\n",
    "    qa['gen_question_flan_t5_xxl'] = generated_question\n",
    "\n",
    "    # Append the updated QA pair to the list\n",
    "    qa_list_updated.append(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 78 elements to qa_list.json.\n"
     ]
    }
   ],
   "source": [
    "# Save the QA updated list to a JSON file\n",
    "save_json(\"qa_list.json\", qa_list_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section deletes the text generation pipeline and the T5 model from memory, triggers Python's garbage collector, and clears the memory cache in PyTorch for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete the 'generate_text' and the 't5_model' objects from memory\n",
    "del generate_text\n",
    "del t5_model\n",
    "\n",
    "# Manually trigger Python's garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear the memory cache in PyTorch for the GPU\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PyTorchCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
